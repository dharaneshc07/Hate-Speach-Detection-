# -*- coding: utf-8 -*-
"""generate_visualizations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n9S5pA3zsyWKkLSRUCIGU4116t7qIs_z
"""

# Import necessary libraries
import pandas as pd  # Ref [1]: McKinney, W., "pandas: a foundational Python library for data analysis and statistics," Python for Data Analysis, 2010.
import matplotlib.pyplot as plt  # Ref [2]: Hunter, J. D., "Matplotlib: A 2D Graphics Environment," Computing in Science & Engineering, vol. 9, no. 3, pp. 90–95, 2007.
from wordcloud import WordCloud  # Ref [3]: Mueller, A., "WordCloud for Python," 2011. [Online]. Available: https://github.com/amueller/word_cloud
import seaborn as sns  # Ref [4]: Waskom, M., "Seaborn: Statistical Data Visualization," Journal of Open Source Software, vol. 6, no. 60, 2021.
import re

# Mount Google Drive to access dataset files
from google.colab import drive
drive.mount('/content/drive')

# Load Dataset
train_dataset_path = "/content/drive/MyDrive/Hate Speech Detection/train_en.tsv"
test_dataset_path = "/content/drive/MyDrive/Hate Speech Detection/test_en.tsv"
df = pd.read_csv(train_dataset_path, sep="\t")  # Ref [5]: Basile, V., et al., "SemEval-2019 Task 5," in Proc. SemEval, 2019, pp. 54–63.
df_test = pd.read_csv(test_dataset_path, sep="\t", header=None)
df_test.columns = ['id', 'text', 'HS', 'TR', 'AG']

# Verify dataset loading
print("Train Dataset Shape:", df.shape)
print("Test Dataset Shape:", df_test.shape)
print("Sample of Train Data:\n", df.head())
print("Sample of Test Data:\n", df_test.head())

# Enhanced Preprocessing (for word cloud generation)
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'[^\w\s#@]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['clean_text'] = df['text'].apply(clean_text)
df_test['clean_text'] = df_test['text'].apply(clean_text)

# Data Exploration: Compute Statistics
# Dataset size
train_size = len(df)
test_size = len(df_test)
print(f"Training set size: {train_size}")
print(f"Test set size: {test_size}")

# Class distribution (HS=0 vs HS=1)
train_class_dist = df['HS'].value_counts().to_dict()
test_class_dist = df_test['HS'].value_counts().to_dict()
print(f"Training set class distribution: HS=0: {train_class_dist[0]}, HS=1: {train_class_dist[1]}")
print(f"Test set class distribution: HS=0: {test_class_dist[0]}, HS=1: {test_class_dist[1]}")

# Missing values
print("Missing Values in Training Set:\n", df.isnull().sum())
print("Missing Values in Test Set:\n", df_test.isnull().sum())

# Word count statistics
df['word_count'] = df['clean_text'].apply(lambda x: len(str(x).split()))
df_test['word_count'] = df_test['clean_text'].apply(lambda x: len(str(x).split()))
train_avg_words = df['word_count'].mean()
test_avg_words = df_test['word_count'].mean()
print(f"Average word count (train): {train_avg_words:.2f}")
print(f"Average word count (test): {test_avg_words:.2f}")

# Generate Visualizations
# Bar chart for class distribution (Figure 1 in the report)
labels = ['Training HS=0', 'Training HS=1', 'Test HS=0', 'Test HS=1']
sizes = [train_class_dist[0], train_class_dist[1], test_class_dist[0], test_class_dist[1]]
plt.figure(figsize=(8, 6))
plt.bar(labels, sizes, color=['blue', 'orange', 'blue', 'orange'])
plt.title('Class Distribution in Training and Test Sets')
plt.ylabel('Number of Samples')
plt.savefig('class_distribution.png')
plt.close()

# Word cloud for hate speech terms (HS=1) (Figure 2 in the report)
hate_speech_tweets = df[df['HS'] == 1]['clean_text'].tolist()
text = " ".join(hate_speech_tweets)
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Hate Speech Terms')
plt.savefig('word_cloud.png')
plt.close()

print("Descriptive analysis completed. Visualizations saved as 'class_distribution.png' and 'word_cloud.png'.")